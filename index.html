---
layout: page
gh-repo:
gh-badge: [star, watch, fork, follow]
share-description:
---
<div class="overlay"></div>
<div class="container">
    <div class="row">
        <div class="col-xl-12 mx-auto text-center">
            <h1>Audio-Visual Voice Separation Transformer</h1>
        </div>
        <div class="col-md-10 col-lg-8 col-xl-7 mx-auto">
        </div>
    </div>
</div>

<br><br>
<div class="col-xl-10 col-lg-8 offset-lg-1">

    <!-- Testimonials -->
    <div class="row justify-content-center">
        <div class="col-sm-3 text-center">
            <a target="_blank"
               href="./"><img src="assets/img/paper.png" width="120" height="130"
                                                            style="border:1px solid black;"></a>
            <h5 style="padding-bottom: 5%; padding-top: 5%">Paper</h5>
        </div>
        <div class="col-sm-3 text-center">
            <a href="./"
               style="color: #242124">
                <i class="fab fa-github fa-8x"></i></a>
            <h5 style="padding-bottom: 5%; padding-top: 5%">Code + Weights</h5>
        </div>
        <div class="col-sm-3 text-center">
            <a style="color: #242124;"
               href="./demos/">
                <i class="fas fa-film fa-8x" style="transform: scale(1,1.275); padding-top: 2.5px"></i>
            </a>
            <h5 style="padding-bottom: 5%; padding-top: 5px">Demos</h5>
        </div>
    </div>
    </br>
    <!-- Image Showcases -->
    <h2 style="text-align: center">Abstract</h2>
    <p class="lead mb-0" align="justify">
        This paper presents an audio-visual approach for voice separation which outperforms state-of-the-
        art methods at a low latency in two scenarios: speech and singing voice. The model is based on
        a two-stage network. Motion cues are obtained with a lightweight graph convolutional network
        that processes face landmarks. Then, both audio and motion features are fed to an audio-visual
        transformer which produces a fairly good estimation of the isolated target source. In a second stage,
        the predominant voice is enhanced with an audio-only network. We present different ablation studies
        and comparison to state-of-the-art methods. Finally, we explore the transferability of models trained
        for speech separation in the task of singing voice separation. The demos, code, and weights will be
        made publicly available at <a href="https://avvst.github.io/">https://avvst.github.io/</a>
    </p>
    </br>


</div>
